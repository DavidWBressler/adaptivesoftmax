{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a step by step walk through a Pytorch implementation of the adaptive softmax from Grave et al.'s 2017 paper, using Pytorchâ€™s built-in AdaptiveLogSoftmaxWithLoss function. I use the wikitext-2 dataset. Check out my blog posts for further explanation:\n",
    "https://towardsdatascience.com/speed-up-your-deep-learning-language-model-up-to-1000-with-the-adaptive-softmax-part-1-e7cc1f89fcc9\n",
    "\n",
    "https://towardsdatascience.com/speed-up-your-deep-learning-language-model-up-to-1000-with-the-adaptive-softmax-part-2-pytorch-d47fe9a56152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install pytorch and fastai (see https://docs.fast.ai/)\n",
    "!conda update conda --yes\n",
    "!conda install -c pytorch pytorch-nightly cuda90 --yes\n",
    "!conda install -c fastai torchvision-nightly --yes\n",
    "!conda install -c fastai fastai --yes\n",
    "!conda install -c anaconda jupyter unzip cython cupy seaborn --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install QRNN\n",
    "    #Follow directions here if QRNN \"encode\" error encountered:\n",
    "    #https://github.com/salesforce/pytorch-qrnn/issues/17\n",
    "    #In /opt/conda/lib/python3.6/site-packages/torchqrnn/forget_mult.py,\n",
    "    #Need to change line 102:\n",
    "    #program = Program(kernel.encode(), 'recurrent_forget_mult.cu'.encode())\n",
    "    #should be: program = Program(kernel, 'recurrent_forget_mult.cu')\n",
    "!pip install pynvrtc git+https://github.com/salesforce/pytorch-qrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import * \n",
    "\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import importlib\n",
    "from torchqrnn import QRNN\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = Path('/data/testwikitext2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-12 23:24:26--  https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.72.106\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.72.106|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4070055 (3.9M) [application/x-tar]\n",
      "Saving to: '/data/wikitext-2.tgz.1'\n",
      "\n",
      "wikitext-2.tgz.1    100%[===================>]   3.88M  1.17MB/s    in 3.3s    \n",
      "\n",
      "2018-11-12 23:24:30 (1.17 MB/s) - '/data/wikitext-2.tgz.1' saved [4070055/4070055]\n",
      "\n",
      "--2018-11-12 23:24:31--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2018-11-12 23:24:31--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: '/data/testwikitext2/glove.6B.zip'\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  1.03MB/s    in 13m 28s \n",
      "\n",
      "2018-11-12 23:38:00 (1.02 MB/s) - '/data/testwikitext2/glove.6B.zip' saved [862182613/862182613]\n",
      "\n",
      "Archive:  /data/testwikitext2/glove.6B.zip\n",
      "  inflating: /data/testwikitext2/glove.6B.50d.txt  \n",
      "  inflating: /data/testwikitext2/glove.6B.100d.txt  \n",
      "  inflating: /data/testwikitext2/glove.6B.200d.txt  \n",
      "  inflating: /data/testwikitext2/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "#download wikitext-2 dataset and GloVe embeddings\n",
    "!wget https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz -P /data\n",
    "!tar xzf /data/wikitext-2.tgz -C /data\n",
    "!mv /data/wikitext-2/ /data/testwikitext2/\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip -P /data/testwikitext2/\n",
    "!unzip /data/testwikitext2/glove.6B.zip -d /data/testwikitext2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put data into csv's w/ columns for 'labels' and 'text'\n",
    "df_trn = pd.read_csv(DATAPATH/'train.csv',header=None,names=['text'])\n",
    "df_test = pd.read_csv(DATAPATH/'test.csv',header=None,names=['text'])\n",
    "df_trn['labels']=0\n",
    "df_test['labels']=0\n",
    "df_trn=df_trn[['labels','text']]\n",
    "df_test=df_test[['labels','text']]\n",
    "df_trn.to_csv(DATAPATH/'train_proc.csv', header=False, index=False)\n",
    "df_test.to_csv(DATAPATH/'test_proc.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create modeler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modeler():\n",
    "    def __init__(self,trn_dl,val_dl,module,criterion,modelvals=None):\n",
    "        self.trn_dl, self.val_dl, self.module = trn_dl, val_dl, module\n",
    "        self.modelvals=modelvals\n",
    "        self.model=self.module.cuda()\n",
    "        self.criterion = criterion\n",
    "    def model_fit(self):\n",
    "        samp_n=self.modelvals['samp_n']#the number of iterations in an epoch\n",
    "        starttime=time.time()\n",
    "        train_loss_list=[]; val_loss_list=[]\n",
    "        for epoch in range(0, self.modelvals['epochs']):\n",
    "            pbar=0#progressbar\n",
    "            for batch_idx, (data, target) in enumerate(self.trn_dl):\n",
    "                \n",
    "                #GRAB MINIBATCH OF INPUTS AND TARGETS, SET OPTIMIZER\n",
    "                data = Variable(data)\n",
    "                pbar+=self.modelvals['bs'] #how many iterations have we done in the epoch so far\n",
    "                if self.modelvals['opttype']=='sgd':\n",
    "                    self.optimizer = optim.SGD(self.model.parameters(), lr=self.modelvals['lr'], \n",
    "                                       momentum=self.modelvals['mom'], weight_decay=self.modelvals['wd'],\n",
    "                                              nesterov=self.modelvals['nesterov'])\n",
    "                elif self.modelvals['opttype']=='adam':\n",
    "                    self.optimizer = optim.Adam(self.model.parameters(), lr=self.modelvals['lr'], \n",
    "                                        betas=(self.modelvals['mom'], 0.999))\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                #FORWARD PASS\n",
    "                output = self.model(data)\n",
    "                \n",
    "                #CALCULATE AND BACKPROP THE LOSS\n",
    "                if self.modelvals['adas']==1:\n",
    "                    loss= output.loss\n",
    "                else:\n",
    "                    target=data[1:,:] #[bptt,bs]\n",
    "                    target=target.view(target.size()[0]*target.size()[1])#[bptt*bs]\n",
    "                    loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                if self.modelvals['grad_clip']!=0: #gradient clipping\n",
    "                    torch.nn.utils.clip_grad_value_(self.model.parameters(), self.modelvals['grad_clip'])\n",
    "                    \n",
    "                #UPDATE THE WEIGHTS\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                #PRINT OUT TRAINING UPDATES\n",
    "                train_loss_list.append([epoch,pbar+epoch*samp_n,loss.data.item(),self.modelvals['lr']])\n",
    "                if batch_idx % 100 == 0:\n",
    "                    elapsed_time=time.time()-starttime\n",
    "                    train_update_format_string = 'Train Epoch: {}'\n",
    "                    train_update_format_string += '\\tTotal_its: {:.2f}M [{:.2f}M/{:.2f}M]'\n",
    "                    train_update_format_string += '\\tPercdone: {:.2f}'\n",
    "                    train_update_format_string += '\\tLoss: {:.4f}'\n",
    "                    train_update_format_string += '\\tTime: {:.2f}'\n",
    "                    train_update_format_string += '\\tLR: {:.4f}'\n",
    "                    train_update_string=train_update_format_string.format(\n",
    "                            epoch,\n",
    "                            (pbar + epoch * samp_n) / 1000000, pbar / 1000000, samp_n / 1000000,\n",
    "                            pbar / samp_n,\n",
    "                            loss.data.item(),\n",
    "                            elapsed_time / 60,\n",
    "                            self.modelvals['lr'])\n",
    "                    print(train_update_string)\n",
    "            final_train_loss=loss.data.item()\n",
    "            \n",
    "            #NOW TEST VALIDATION SET\n",
    "            val_loss=[]\n",
    "            self.model.eval() #important to set to eval mode for testing, so that eg batchnorm and dropout aren't used\n",
    "            for batch_idx, (data, target) in enumerate(self.val_dl):\n",
    "                data = Variable(data)\n",
    "                self.optimizer.zero_grad()\n",
    "                #ONLY NEED FORWARD PASS... NO BACKPROP\n",
    "                output = self.model(data)\n",
    "                if self.modelvals['adas']==1:\n",
    "                    loss= output.loss\n",
    "                    output=output.output\n",
    "                else:\n",
    "                    target=data[1:,:] #[bptt,bs]\n",
    "                    target=target.view(target.size()[0]*target.size()[1])#[bptt*bs]\n",
    "                    loss = self.criterion(output, target)\n",
    "                val_loss.append(loss.data.item())\n",
    "            self.model.train() #set back to training mode\n",
    "            ave_val_loss=sum(val_loss) / len(val_loss)\n",
    "            val_update_string='Validation Loss: {:.4f}\\tPerp: {:.4f}'.format(\n",
    "                ave_val_loss,np.exp(ave_val_loss))\n",
    "            print(val_update_string)\n",
    "            val_loss_list.append([epoch,ave_val_loss, np.exp(ave_val_loss),elapsed_time/60])\n",
    "        self.modelvals['val_loss_list']=val_loss_list\n",
    "        self.modelvals['train_loss_list']=train_loss_list\n",
    "        print('The end! {:.2f} minutes'.format((time.time()-starttime)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters and build embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyperparameters\n",
    "adas=1 #set to 1 for adaptive softmax. set to 0 for regular softmax\n",
    "bs=50 #batch-size\n",
    "bptt=70 #the average sequence-length for our inputs. There's some variance around this number\n",
    "emb_sz=300 #size of the embedding matrix\n",
    "nl=4 #number of layers\n",
    "nh=300 #number hidden units\n",
    "lr=1 #learning rate\n",
    "mom=.99 #momentum\n",
    "wd=1e-7 #weight-decay. Only has effect if opttype==sgd\n",
    "epochs=45\n",
    "criterion = nn.NLLLoss() #negative log-likelihood loss\n",
    "nesterov=True #Nesterov momentum. only has effect if opttype==sgd\n",
    "grad_clip=0.1 #gradient clipping value. Set to 0 for no effect. See nn.utils.clip_grad_value_\n",
    "opttype='sgd' #adam, sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use fast.ai to create a TextLMDataBunch object. See http://docs.fast.ai/text.data.html#class-textlmdatabunch\n",
    "#This tokenizes and numericalizes the data, and puts the data into a handy data-loader\n",
    "#data_lm.train_dl is the data-loader we will be using\n",
    "data_lm = TextLMDataBunch.from_csv(path=DATAPATH, csv_name='train_proc.csv', test='test_proc.csv', bs=bs,bptt=bptt)\n",
    "itos=data_lm.train_ds.vocab.itos# the vocab\n",
    "vs=len(itos)# vs is the length of the vocab: 25520\n",
    "samp_n=len(data_lm.train_dl)*bs #total number of iterations... i.e. number_of_batches * batch_size\n",
    "val_samp_n=len(data_lm.valid_dl)*bs #total number of iterations... i.e. number_of_batches * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put hyperparameters into a dictionary\n",
    "def get_modelvals():\n",
    "    modelvals=dict((name,eval(name)) for name in [\n",
    "        'lr','mom','wd','opttype','epochs','bptt','samp_n','val_samp_n',\n",
    "        'bs','emb_sz','vs', 'nh', 'nl','DATAPATH','adas','nesterov','grad_clip'] )\n",
    "    modelvals['criterion_str']=str(criterion)\n",
    "    return modelvals\n",
    "\n",
    "modelvals=get_modelvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab GloVe embeddings:\n",
    "#create vocab itos2 from downloaded glove file\n",
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = []\n",
    "with open('/data/testwikitext2/glove.6B.300d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vectors.append(line[1:])\n",
    "itos2=words\n",
    "\n",
    "#grab the glove embeddings we need, based on the words in our vocab\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)}) #default -1 means its not in glove's itos2\n",
    "row_m = vectors[-1] #this is default vector... for <unk>\n",
    "new_w = np.zeros((vs, emb_sz), dtype=np.float32)#initialize new weights to zeros of size (vocab_size,embedding size) e.g. (60002,300)... we're creating an embedding matrix \n",
    "for i,w in enumerate(itos): #for index,word in our itos dict, get r index of the word in word2vec's dict. r will be -1 if it doesn't exist in word2vec's dict\n",
    "    r = stoi2[w]#r index of the word in word2vec's dict\n",
    "    new_w[i] = vectors[r] if r>=0 else row_m #for our new embedding matrix, set the embedding at the index from our dict equal to the embedding from index r from word2vec's dict\n",
    "np.save(DATAPATH/'emb_wgts300_proc.npy', new_w) #save the embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run QRNN-network with regular softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a simple RNNnet w/ regular softmax\n",
    "class QRNNnet(nn.Module):\n",
    "    def __init__(self, vs, emb_sz, nh, nl,bs):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vs, emb_sz)\n",
    "        self.rnn = QRNN(emb_sz, nh, num_layers=nl)\n",
    "        self.rnn.cuda()\n",
    "        self.hidden=self.one_hidden()\n",
    "        self.fc1 = nn.Linear(nh, vs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #set x to [:-1] of the sequence (i.e. all but the last value)\n",
    "        x=x[:-1,:]#[bptt,bs]\n",
    "        x = self.embed(x) # x -> [bptt,bs] -> [bptt,bs,emb_sz] \n",
    "        x,new_h = self.rnn(x, self.hidden) #[bptt,bs,emb_sz]  -> [bptt,bs,nh]\n",
    "        self.hidden = Variable(new_h.data)#repackage\n",
    "        x= self.fc1(x)#[bptt,bs,nh] -> [bptt,bs,vs]\n",
    "        return F.log_softmax(x.view(-1,vs),dim=1) #[bptt,bs,vs] -> [bptt*bs,vs]\n",
    "    \n",
    "    def one_hidden(self,nl,bs,nh):\n",
    "        hidden1=Variable(torch.zeros(nl, bs, nh).cuda(), requires_grad=False)\n",
    "        return hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QRNNnet(\n",
      "  (embed): Embedding(25520, 300)\n",
      "  (rnn): QRNN(\n",
      "    (layers): ModuleList(\n",
      "      (0): QRNNLayer(\n",
      "        (linear): Linear(in_features=300, out_features=900, bias=True)\n",
      "      )\n",
      "      (1): QRNNLayer(\n",
      "        (linear): Linear(in_features=300, out_features=900, bias=True)\n",
      "      )\n",
      "      (2): QRNNLayer(\n",
      "        (linear): Linear(in_features=300, out_features=900, bias=True)\n",
      "      )\n",
      "      (3): QRNNLayer(\n",
      "        (linear): Linear(in_features=300, out_features=900, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=300, out_features=25520, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#create QRNN network w/ traditional softmax\n",
    "qnet=modeler(data_lm.train_dl,data_lm.valid_dl,\n",
    "                           QRNNnet(vs, emb_sz,nh,nl,bs),criterion,modelvals)\n",
    "print(qnet.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the glove-vectors into our model\n",
    "new_w=np.load(DATAPATH/'emb_wgts300_proc.npy') #load embedding weights\n",
    "qnet.model.embed.weight.data=torch.FloatTensor(new_w).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1131: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1143: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tTotal_its: 0.00M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 10.1461\tTime: 0.00\tLR: 1.0000\n",
      "Train Epoch: 0\tTotal_its: 0.01M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 7.7374\tTime: 0.10\tLR: 1.0000\n",
      "Train Epoch: 0\tTotal_its: 0.01M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 7.0925\tTime: 0.21\tLR: 1.0000\n",
      "Train Epoch: 0\tTotal_its: 0.02M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 7.2240\tTime: 0.31\tLR: 1.0000\n",
      "Train Epoch: 0\tTotal_its: 0.02M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.9955\tTime: 0.42\tLR: 1.0000\n",
      "Validation Loss: 6.6812\tPerp: 797.3000\n",
      "Train Epoch: 1\tTotal_its: 0.02M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.9558\tTime: 0.54\tLR: 1.0000\n",
      "Train Epoch: 1\tTotal_its: 0.03M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 7.1057\tTime: 0.65\tLR: 1.0000\n",
      "Train Epoch: 1\tTotal_its: 0.03M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 7.0527\tTime: 0.75\tLR: 1.0000\n",
      "Train Epoch: 1\tTotal_its: 0.04M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 6.7982\tTime: 0.85\tLR: 1.0000\n",
      "Train Epoch: 1\tTotal_its: 0.04M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.8777\tTime: 0.96\tLR: 1.0000\n",
      "Validation Loss: 6.5674\tPerp: 711.5033\n",
      "Train Epoch: 2\tTotal_its: 0.05M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.8664\tTime: 1.08\tLR: 1.0000\n",
      "Train Epoch: 2\tTotal_its: 0.05M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 6.7669\tTime: 1.19\tLR: 1.0000\n",
      "Train Epoch: 2\tTotal_its: 0.06M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 6.7549\tTime: 1.29\tLR: 1.0000\n",
      "Train Epoch: 2\tTotal_its: 0.06M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 6.9498\tTime: 1.40\tLR: 1.0000\n",
      "Train Epoch: 2\tTotal_its: 0.07M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.6321\tTime: 1.50\tLR: 1.0000\n",
      "Validation Loss: 6.3793\tPerp: 589.4874\n",
      "Train Epoch: 3\tTotal_its: 0.07M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.6722\tTime: 1.63\tLR: 1.0000\n",
      "Train Epoch: 3\tTotal_its: 0.08M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 6.7663\tTime: 1.74\tLR: 1.0000\n",
      "Train Epoch: 3\tTotal_its: 0.08M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 6.6635\tTime: 1.84\tLR: 1.0000\n",
      "Train Epoch: 3\tTotal_its: 0.09M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 6.3928\tTime: 1.95\tLR: 1.0000\n",
      "Train Epoch: 3\tTotal_its: 0.09M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.4853\tTime: 2.05\tLR: 1.0000\n",
      "Validation Loss: 6.2415\tPerp: 513.6079\n",
      "Train Epoch: 4\tTotal_its: 0.10M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.4219\tTime: 2.18\tLR: 1.0000\n",
      "Train Epoch: 4\tTotal_its: 0.10M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 6.5404\tTime: 2.28\tLR: 1.0000\n",
      "Train Epoch: 4\tTotal_its: 0.11M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 6.3291\tTime: 2.39\tLR: 1.0000\n",
      "Train Epoch: 4\tTotal_its: 0.11M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 6.2835\tTime: 2.49\tLR: 1.0000\n",
      "Train Epoch: 4\tTotal_its: 0.12M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.1460\tTime: 2.59\tLR: 1.0000\n",
      "Validation Loss: 6.0267\tPerp: 414.3466\n",
      "Train Epoch: 5\tTotal_its: 0.12M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.3088\tTime: 2.72\tLR: 1.0000\n",
      "Train Epoch: 5\tTotal_its: 0.13M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 6.2192\tTime: 2.82\tLR: 1.0000\n",
      "Train Epoch: 5\tTotal_its: 0.13M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.9948\tTime: 2.93\tLR: 1.0000\n",
      "Train Epoch: 5\tTotal_its: 0.14M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 6.1278\tTime: 3.04\tLR: 1.0000\n",
      "Train Epoch: 5\tTotal_its: 0.14M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.0721\tTime: 3.14\tLR: 1.0000\n",
      "Validation Loss: 5.7999\tPerp: 330.2691\n",
      "Train Epoch: 6\tTotal_its: 0.14M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.1373\tTime: 3.27\tLR: 1.0000\n",
      "Train Epoch: 6\tTotal_its: 0.15M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.9583\tTime: 3.37\tLR: 1.0000\n",
      "Train Epoch: 6\tTotal_its: 0.15M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.9459\tTime: 3.48\tLR: 1.0000\n",
      "Train Epoch: 6\tTotal_its: 0.16M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.9538\tTime: 3.58\tLR: 1.0000\n",
      "Train Epoch: 6\tTotal_its: 0.16M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.9911\tTime: 3.69\tLR: 1.0000\n",
      "Validation Loss: 5.7053\tPerp: 300.4614\n",
      "Train Epoch: 7\tTotal_its: 0.17M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.9472\tTime: 3.81\tLR: 1.0000\n",
      "Train Epoch: 7\tTotal_its: 0.17M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.8638\tTime: 3.92\tLR: 1.0000\n",
      "Train Epoch: 7\tTotal_its: 0.18M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.7840\tTime: 4.03\tLR: 1.0000\n",
      "Train Epoch: 7\tTotal_its: 0.18M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.7772\tTime: 4.13\tLR: 1.0000\n",
      "Train Epoch: 7\tTotal_its: 0.19M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.8298\tTime: 4.24\tLR: 1.0000\n",
      "Validation Loss: 5.5350\tPerp: 253.4041\n",
      "Train Epoch: 8\tTotal_its: 0.19M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.7806\tTime: 4.36\tLR: 1.0000\n",
      "Train Epoch: 8\tTotal_its: 0.20M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.5798\tTime: 4.47\tLR: 1.0000\n",
      "Train Epoch: 8\tTotal_its: 0.20M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.6061\tTime: 4.57\tLR: 1.0000\n",
      "Train Epoch: 8\tTotal_its: 0.21M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.6418\tTime: 4.67\tLR: 1.0000\n",
      "Train Epoch: 8\tTotal_its: 0.21M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.6144\tTime: 4.78\tLR: 1.0000\n",
      "Validation Loss: 5.4808\tPerp: 240.0317\n",
      "Train Epoch: 9\tTotal_its: 0.22M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.5824\tTime: 4.90\tLR: 1.0000\n",
      "Train Epoch: 9\tTotal_its: 0.22M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.6269\tTime: 5.01\tLR: 1.0000\n",
      "Train Epoch: 9\tTotal_its: 0.23M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.6274\tTime: 5.12\tLR: 1.0000\n",
      "Train Epoch: 9\tTotal_its: 0.23M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.5865\tTime: 5.22\tLR: 1.0000\n",
      "Train Epoch: 9\tTotal_its: 0.24M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.4735\tTime: 5.33\tLR: 1.0000\n",
      "Validation Loss: 5.4341\tPerp: 229.0833\n",
      "Train Epoch: 10\tTotal_its: 0.24M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.6281\tTime: 5.45\tLR: 1.0000\n",
      "Train Epoch: 10\tTotal_its: 0.25M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.2960\tTime: 5.56\tLR: 1.0000\n",
      "Train Epoch: 10\tTotal_its: 0.25M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.3927\tTime: 5.66\tLR: 1.0000\n",
      "Train Epoch: 10\tTotal_its: 0.26M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.5293\tTime: 5.76\tLR: 1.0000\n",
      "Train Epoch: 10\tTotal_its: 0.26M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.3555\tTime: 5.87\tLR: 1.0000\n",
      "Validation Loss: 5.3603\tPerp: 212.7889\n",
      "Train Epoch: 11\tTotal_its: 0.26M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.6443\tTime: 5.99\tLR: 1.0000\n",
      "Train Epoch: 11\tTotal_its: 0.27M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.3941\tTime: 6.10\tLR: 1.0000\n",
      "Train Epoch: 11\tTotal_its: 0.27M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.4694\tTime: 6.20\tLR: 1.0000\n",
      "Train Epoch: 11\tTotal_its: 0.28M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.3336\tTime: 6.31\tLR: 1.0000\n",
      "Train Epoch: 11\tTotal_its: 0.28M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.4279\tTime: 6.41\tLR: 1.0000\n",
      "Validation Loss: 5.3080\tPerp: 201.9425\n",
      "Train Epoch: 12\tTotal_its: 0.29M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.5384\tTime: 6.54\tLR: 1.0000\n",
      "Train Epoch: 12\tTotal_its: 0.29M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.2492\tTime: 6.64\tLR: 1.0000\n",
      "Train Epoch: 12\tTotal_its: 0.30M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.4812\tTime: 6.75\tLR: 1.0000\n",
      "Train Epoch: 12\tTotal_its: 0.30M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.2834\tTime: 6.85\tLR: 1.0000\n",
      "Train Epoch: 12\tTotal_its: 0.31M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.3534\tTime: 6.96\tLR: 1.0000\n",
      "Validation Loss: 5.2601\tPerp: 192.5073\n",
      "Train Epoch: 13\tTotal_its: 0.31M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.2734\tTime: 7.08\tLR: 1.0000\n",
      "Train Epoch: 13\tTotal_its: 0.32M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.3283\tTime: 7.19\tLR: 1.0000\n",
      "Train Epoch: 13\tTotal_its: 0.32M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.2876\tTime: 7.29\tLR: 1.0000\n",
      "Train Epoch: 13\tTotal_its: 0.33M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.2412\tTime: 7.40\tLR: 1.0000\n",
      "Train Epoch: 13\tTotal_its: 0.33M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.2318\tTime: 7.50\tLR: 1.0000\n",
      "Validation Loss: 5.2226\tPerp: 185.4233\n",
      "Train Epoch: 14\tTotal_its: 0.34M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.4140\tTime: 7.63\tLR: 1.0000\n",
      "Train Epoch: 14\tTotal_its: 0.34M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.1704\tTime: 7.73\tLR: 1.0000\n",
      "Train Epoch: 14\tTotal_its: 0.35M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.1812\tTime: 7.84\tLR: 1.0000\n",
      "Train Epoch: 14\tTotal_its: 0.35M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.1066\tTime: 7.95\tLR: 1.0000\n",
      "Train Epoch: 14\tTotal_its: 0.36M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.2766\tTime: 8.05\tLR: 1.0000\n",
      "Validation Loss: 5.1867\tPerp: 178.8829\n",
      "Train Epoch: 15\tTotal_its: 0.36M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.3730\tTime: 8.18\tLR: 1.0000\n",
      "Train Epoch: 15\tTotal_its: 0.37M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.0318\tTime: 8.28\tLR: 1.0000\n",
      "Train Epoch: 15\tTotal_its: 0.37M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.0679\tTime: 8.39\tLR: 1.0000\n",
      "Train Epoch: 15\tTotal_its: 0.38M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.1232\tTime: 8.50\tLR: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15\tTotal_its: 0.38M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.1001\tTime: 8.60\tLR: 1.0000\n",
      "Validation Loss: 5.1563\tPerp: 173.5189\n",
      "Train Epoch: 16\tTotal_its: 0.38M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.3429\tTime: 8.73\tLR: 1.0000\n",
      "Train Epoch: 16\tTotal_its: 0.39M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.8841\tTime: 8.83\tLR: 1.0000\n",
      "Train Epoch: 16\tTotal_its: 0.39M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.0908\tTime: 8.94\tLR: 1.0000\n",
      "Train Epoch: 16\tTotal_its: 0.40M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.0018\tTime: 9.04\tLR: 1.0000\n",
      "Train Epoch: 16\tTotal_its: 0.40M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.9580\tTime: 9.15\tLR: 1.0000\n",
      "Validation Loss: 5.1229\tPerp: 167.8284\n",
      "Train Epoch: 17\tTotal_its: 0.41M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.2265\tTime: 9.27\tLR: 1.0000\n",
      "Train Epoch: 17\tTotal_its: 0.41M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.1426\tTime: 9.37\tLR: 1.0000\n",
      "Train Epoch: 17\tTotal_its: 0.42M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.1062\tTime: 9.48\tLR: 1.0000\n",
      "Train Epoch: 17\tTotal_its: 0.42M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.9424\tTime: 9.59\tLR: 1.0000\n",
      "Train Epoch: 17\tTotal_its: 0.43M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.0337\tTime: 9.69\tLR: 1.0000\n",
      "Validation Loss: 5.1023\tPerp: 164.4044\n",
      "Train Epoch: 18\tTotal_its: 0.43M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.0728\tTime: 9.82\tLR: 1.0000\n",
      "Train Epoch: 18\tTotal_its: 0.44M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.0889\tTime: 9.92\tLR: 1.0000\n",
      "Train Epoch: 18\tTotal_its: 0.44M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.9667\tTime: 10.03\tLR: 1.0000\n",
      "Train Epoch: 18\tTotal_its: 0.45M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.9793\tTime: 10.13\tLR: 1.0000\n",
      "Train Epoch: 18\tTotal_its: 0.45M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.9654\tTime: 10.24\tLR: 1.0000\n",
      "Validation Loss: 5.0803\tPerp: 160.8200\n",
      "Train Epoch: 19\tTotal_its: 0.46M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.1338\tTime: 10.36\tLR: 1.0000\n",
      "Train Epoch: 19\tTotal_its: 0.46M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.0290\tTime: 10.46\tLR: 1.0000\n",
      "Train Epoch: 19\tTotal_its: 0.47M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.9358\tTime: 10.57\tLR: 1.0000\n",
      "Train Epoch: 19\tTotal_its: 0.47M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.9301\tTime: 10.67\tLR: 1.0000\n",
      "Train Epoch: 19\tTotal_its: 0.48M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.9559\tTime: 10.78\tLR: 1.0000\n",
      "Validation Loss: 5.0561\tPerp: 156.9748\n",
      "Train Epoch: 20\tTotal_its: 0.48M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.1677\tTime: 10.91\tLR: 1.0000\n",
      "Train Epoch: 20\tTotal_its: 0.49M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.8791\tTime: 11.01\tLR: 1.0000\n",
      "Train Epoch: 20\tTotal_its: 0.49M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.8749\tTime: 11.12\tLR: 1.0000\n",
      "Train Epoch: 20\tTotal_its: 0.50M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.8723\tTime: 11.22\tLR: 1.0000\n",
      "Train Epoch: 20\tTotal_its: 0.50M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.9544\tTime: 11.32\tLR: 1.0000\n",
      "Validation Loss: 5.0431\tPerp: 154.9492\n",
      "Train Epoch: 21\tTotal_its: 0.50M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.2081\tTime: 11.45\tLR: 1.0000\n",
      "Train Epoch: 21\tTotal_its: 0.51M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.0206\tTime: 11.55\tLR: 1.0000\n",
      "Train Epoch: 21\tTotal_its: 0.51M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.7828\tTime: 11.66\tLR: 1.0000\n",
      "Train Epoch: 21\tTotal_its: 0.52M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.8156\tTime: 11.76\tLR: 1.0000\n",
      "Train Epoch: 21\tTotal_its: 0.52M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.8705\tTime: 11.87\tLR: 1.0000\n",
      "Validation Loss: 5.0210\tPerp: 151.5601\n",
      "Train Epoch: 22\tTotal_its: 0.53M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.1874\tTime: 12.00\tLR: 1.0000\n",
      "Train Epoch: 22\tTotal_its: 0.53M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.9504\tTime: 12.10\tLR: 1.0000\n",
      "Train Epoch: 22\tTotal_its: 0.54M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.7217\tTime: 12.21\tLR: 1.0000\n",
      "Train Epoch: 22\tTotal_its: 0.54M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.8467\tTime: 12.31\tLR: 1.0000\n",
      "Train Epoch: 22\tTotal_its: 0.55M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.9965\tTime: 12.42\tLR: 1.0000\n",
      "Validation Loss: 5.0072\tPerp: 149.4793\n",
      "Train Epoch: 23\tTotal_its: 0.55M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.0836\tTime: 12.54\tLR: 1.0000\n",
      "Train Epoch: 23\tTotal_its: 0.56M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.8381\tTime: 12.65\tLR: 1.0000\n",
      "Train Epoch: 23\tTotal_its: 0.56M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.7135\tTime: 12.75\tLR: 1.0000\n",
      "Train Epoch: 23\tTotal_its: 0.57M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.8143\tTime: 12.86\tLR: 1.0000\n",
      "Train Epoch: 23\tTotal_its: 0.57M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.7647\tTime: 12.96\tLR: 1.0000\n",
      "Validation Loss: 4.9815\tPerp: 145.6895\n",
      "Train Epoch: 24\tTotal_its: 0.58M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.9846\tTime: 13.08\tLR: 1.0000\n",
      "Train Epoch: 24\tTotal_its: 0.58M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.7586\tTime: 13.19\tLR: 1.0000\n",
      "Train Epoch: 24\tTotal_its: 0.59M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.7156\tTime: 13.29\tLR: 1.0000\n",
      "Train Epoch: 24\tTotal_its: 0.59M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.7340\tTime: 13.40\tLR: 1.0000\n",
      "Train Epoch: 24\tTotal_its: 0.60M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.8011\tTime: 13.50\tLR: 1.0000\n",
      "Validation Loss: 4.9781\tPerp: 145.1989\n",
      "Train Epoch: 25\tTotal_its: 0.60M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.0303\tTime: 13.63\tLR: 1.0000\n",
      "Train Epoch: 25\tTotal_its: 0.61M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.5714\tTime: 13.74\tLR: 1.0000\n",
      "Train Epoch: 25\tTotal_its: 0.61M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.7387\tTime: 13.84\tLR: 1.0000\n",
      "Train Epoch: 25\tTotal_its: 0.62M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.6452\tTime: 13.95\tLR: 1.0000\n",
      "Train Epoch: 25\tTotal_its: 0.62M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.7467\tTime: 14.05\tLR: 1.0000\n",
      "Validation Loss: 4.9586\tPerp: 142.3981\n",
      "Train Epoch: 26\tTotal_its: 0.62M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.1085\tTime: 14.18\tLR: 1.0000\n",
      "Train Epoch: 26\tTotal_its: 0.63M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.5963\tTime: 14.29\tLR: 1.0000\n",
      "Train Epoch: 26\tTotal_its: 0.63M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.7112\tTime: 14.39\tLR: 1.0000\n",
      "Train Epoch: 26\tTotal_its: 0.64M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.7559\tTime: 14.50\tLR: 1.0000\n",
      "Train Epoch: 26\tTotal_its: 0.64M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5181\tTime: 14.60\tLR: 1.0000\n",
      "Validation Loss: 4.9588\tPerp: 142.4276\n",
      "Train Epoch: 27\tTotal_its: 0.65M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.0489\tTime: 14.73\tLR: 1.0000\n",
      "Train Epoch: 27\tTotal_its: 0.65M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.5907\tTime: 14.83\tLR: 1.0000\n",
      "Train Epoch: 27\tTotal_its: 0.66M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.4957\tTime: 14.94\tLR: 1.0000\n",
      "Train Epoch: 27\tTotal_its: 0.66M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.7657\tTime: 15.05\tLR: 1.0000\n",
      "Train Epoch: 27\tTotal_its: 0.67M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5823\tTime: 15.15\tLR: 1.0000\n",
      "Validation Loss: 4.9303\tPerp: 138.4245\n",
      "Train Epoch: 28\tTotal_its: 0.67M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.9303\tTime: 15.28\tLR: 1.0000\n",
      "Train Epoch: 28\tTotal_its: 0.68M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.6649\tTime: 15.39\tLR: 1.0000\n",
      "Train Epoch: 28\tTotal_its: 0.68M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.6121\tTime: 15.49\tLR: 1.0000\n",
      "Train Epoch: 28\tTotal_its: 0.69M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.6691\tTime: 15.59\tLR: 1.0000\n",
      "Train Epoch: 28\tTotal_its: 0.69M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.6897\tTime: 15.70\tLR: 1.0000\n",
      "Validation Loss: 4.9220\tPerp: 137.2797\n",
      "Train Epoch: 29\tTotal_its: 0.70M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.0766\tTime: 15.82\tLR: 1.0000\n",
      "Train Epoch: 29\tTotal_its: 0.70M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.5870\tTime: 15.93\tLR: 1.0000\n",
      "Train Epoch: 29\tTotal_its: 0.71M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.6650\tTime: 16.04\tLR: 1.0000\n",
      "Train Epoch: 29\tTotal_its: 0.71M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.6145\tTime: 16.14\tLR: 1.0000\n",
      "Train Epoch: 29\tTotal_its: 0.72M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.7315\tTime: 16.24\tLR: 1.0000\n",
      "Validation Loss: 4.9160\tPerp: 136.4570\n",
      "Train Epoch: 30\tTotal_its: 0.72M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.9593\tTime: 16.37\tLR: 1.0000\n",
      "Train Epoch: 30\tTotal_its: 0.73M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.5905\tTime: 16.47\tLR: 1.0000\n",
      "Train Epoch: 30\tTotal_its: 0.73M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.5748\tTime: 16.58\tLR: 1.0000\n",
      "Train Epoch: 30\tTotal_its: 0.74M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.6685\tTime: 16.68\tLR: 1.0000\n",
      "Train Epoch: 30\tTotal_its: 0.74M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.6689\tTime: 16.79\tLR: 1.0000\n",
      "Validation Loss: 4.9144\tPerp: 136.2375\n",
      "Train Epoch: 31\tTotal_its: 0.74M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.8522\tTime: 16.92\tLR: 1.0000\n",
      "Train Epoch: 31\tTotal_its: 0.75M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.5512\tTime: 17.02\tLR: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 31\tTotal_its: 0.75M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.4932\tTime: 17.13\tLR: 1.0000\n",
      "Train Epoch: 31\tTotal_its: 0.76M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4531\tTime: 17.23\tLR: 1.0000\n",
      "Train Epoch: 31\tTotal_its: 0.76M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.6006\tTime: 17.34\tLR: 1.0000\n",
      "Validation Loss: 4.9090\tPerp: 135.5075\n",
      "Train Epoch: 32\tTotal_its: 0.77M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.7522\tTime: 17.46\tLR: 1.0000\n",
      "Train Epoch: 32\tTotal_its: 0.77M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.5427\tTime: 17.57\tLR: 1.0000\n",
      "Train Epoch: 32\tTotal_its: 0.78M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.5499\tTime: 17.68\tLR: 1.0000\n",
      "Train Epoch: 32\tTotal_its: 0.78M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4926\tTime: 17.78\tLR: 1.0000\n",
      "Train Epoch: 32\tTotal_its: 0.79M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.4302\tTime: 17.89\tLR: 1.0000\n",
      "Validation Loss: 4.8928\tPerp: 133.3275\n",
      "Train Epoch: 33\tTotal_its: 0.79M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.6654\tTime: 18.01\tLR: 1.0000\n",
      "Train Epoch: 33\tTotal_its: 0.80M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.5072\tTime: 18.12\tLR: 1.0000\n",
      "Train Epoch: 33\tTotal_its: 0.80M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.3941\tTime: 18.22\tLR: 1.0000\n",
      "Train Epoch: 33\tTotal_its: 0.81M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.5287\tTime: 18.32\tLR: 1.0000\n",
      "Train Epoch: 33\tTotal_its: 0.81M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5804\tTime: 18.43\tLR: 1.0000\n",
      "Validation Loss: 4.8894\tPerp: 132.8764\n",
      "Train Epoch: 34\tTotal_its: 0.82M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.9557\tTime: 18.56\tLR: 1.0000\n",
      "Train Epoch: 34\tTotal_its: 0.82M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.4778\tTime: 18.66\tLR: 1.0000\n",
      "Train Epoch: 34\tTotal_its: 0.83M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.5480\tTime: 18.77\tLR: 1.0000\n",
      "Train Epoch: 34\tTotal_its: 0.83M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.6014\tTime: 18.87\tLR: 1.0000\n",
      "Train Epoch: 34\tTotal_its: 0.84M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.4513\tTime: 18.98\tLR: 1.0000\n",
      "Validation Loss: 4.8766\tPerp: 131.1896\n",
      "Train Epoch: 35\tTotal_its: 0.84M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.6974\tTime: 19.10\tLR: 1.0000\n",
      "Train Epoch: 35\tTotal_its: 0.85M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.4336\tTime: 19.20\tLR: 1.0000\n",
      "Train Epoch: 35\tTotal_its: 0.85M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.5263\tTime: 19.31\tLR: 1.0000\n",
      "Train Epoch: 35\tTotal_its: 0.86M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4669\tTime: 19.41\tLR: 1.0000\n",
      "Train Epoch: 35\tTotal_its: 0.86M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.4387\tTime: 19.52\tLR: 1.0000\n",
      "Validation Loss: 4.8621\tPerp: 129.2891\n",
      "Train Epoch: 36\tTotal_its: 0.86M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.7242\tTime: 19.64\tLR: 1.0000\n",
      "Train Epoch: 36\tTotal_its: 0.87M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.3929\tTime: 19.75\tLR: 1.0000\n",
      "Train Epoch: 36\tTotal_its: 0.87M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.4498\tTime: 19.86\tLR: 1.0000\n",
      "Train Epoch: 36\tTotal_its: 0.88M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4454\tTime: 19.96\tLR: 1.0000\n",
      "Train Epoch: 36\tTotal_its: 0.88M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5720\tTime: 20.07\tLR: 1.0000\n",
      "Validation Loss: 4.8651\tPerp: 129.6842\n",
      "Train Epoch: 37\tTotal_its: 0.89M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.7609\tTime: 20.19\tLR: 1.0000\n",
      "Train Epoch: 37\tTotal_its: 0.89M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.3706\tTime: 20.29\tLR: 1.0000\n",
      "Train Epoch: 37\tTotal_its: 0.90M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.4918\tTime: 20.40\tLR: 1.0000\n",
      "Train Epoch: 37\tTotal_its: 0.90M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4107\tTime: 20.50\tLR: 1.0000\n",
      "Train Epoch: 37\tTotal_its: 0.91M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5004\tTime: 20.61\tLR: 1.0000\n",
      "Validation Loss: 4.8626\tPerp: 129.3585\n",
      "Train Epoch: 38\tTotal_its: 0.91M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.7386\tTime: 20.73\tLR: 1.0000\n",
      "Train Epoch: 38\tTotal_its: 0.92M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.3593\tTime: 20.84\tLR: 1.0000\n",
      "Train Epoch: 38\tTotal_its: 0.92M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.5008\tTime: 20.94\tLR: 1.0000\n",
      "Train Epoch: 38\tTotal_its: 0.93M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.3368\tTime: 21.05\tLR: 1.0000\n",
      "Train Epoch: 38\tTotal_its: 0.93M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.3286\tTime: 21.15\tLR: 1.0000\n",
      "Validation Loss: 4.8598\tPerp: 128.9933\n",
      "Train Epoch: 39\tTotal_its: 0.94M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.5935\tTime: 21.28\tLR: 1.0000\n",
      "Train Epoch: 39\tTotal_its: 0.94M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.4671\tTime: 21.39\tLR: 1.0000\n",
      "Train Epoch: 39\tTotal_its: 0.95M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.2586\tTime: 21.49\tLR: 1.0000\n",
      "Train Epoch: 39\tTotal_its: 0.95M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.3892\tTime: 21.60\tLR: 1.0000\n",
      "Train Epoch: 39\tTotal_its: 0.96M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5549\tTime: 21.71\tLR: 1.0000\n",
      "Validation Loss: 4.8548\tPerp: 128.3563\n",
      "Train Epoch: 40\tTotal_its: 0.96M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.6699\tTime: 21.83\tLR: 1.0000\n",
      "Train Epoch: 40\tTotal_its: 0.97M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.3313\tTime: 21.93\tLR: 1.0000\n",
      "Train Epoch: 40\tTotal_its: 0.97M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.3644\tTime: 22.04\tLR: 1.0000\n",
      "Train Epoch: 40\tTotal_its: 0.98M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4463\tTime: 22.14\tLR: 1.0000\n",
      "Train Epoch: 40\tTotal_its: 0.98M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.3491\tTime: 22.25\tLR: 1.0000\n",
      "Validation Loss: 4.8615\tPerp: 129.2139\n",
      "Train Epoch: 41\tTotal_its: 0.98M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.6484\tTime: 22.37\tLR: 1.0000\n",
      "Train Epoch: 41\tTotal_its: 0.99M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.3541\tTime: 22.48\tLR: 1.0000\n",
      "Train Epoch: 41\tTotal_its: 0.99M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.4170\tTime: 22.59\tLR: 1.0000\n",
      "Train Epoch: 41\tTotal_its: 1.00M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.3548\tTime: 22.69\tLR: 1.0000\n",
      "Train Epoch: 41\tTotal_its: 1.00M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.2948\tTime: 22.80\tLR: 1.0000\n",
      "Validation Loss: 4.8462\tPerp: 127.2498\n",
      "Train Epoch: 42\tTotal_its: 1.01M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.7681\tTime: 22.92\tLR: 1.0000\n",
      "Train Epoch: 42\tTotal_its: 1.01M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.3491\tTime: 23.02\tLR: 1.0000\n",
      "Train Epoch: 42\tTotal_its: 1.02M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.3906\tTime: 23.13\tLR: 1.0000\n",
      "Train Epoch: 42\tTotal_its: 1.02M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.2635\tTime: 23.23\tLR: 1.0000\n",
      "Train Epoch: 42\tTotal_its: 1.03M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.3070\tTime: 23.34\tLR: 1.0000\n",
      "Validation Loss: 4.8493\tPerp: 127.6516\n",
      "Train Epoch: 43\tTotal_its: 1.03M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.6041\tTime: 23.47\tLR: 1.0000\n",
      "Train Epoch: 43\tTotal_its: 1.04M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.3166\tTime: 23.57\tLR: 1.0000\n",
      "Train Epoch: 43\tTotal_its: 1.04M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.3514\tTime: 23.68\tLR: 1.0000\n",
      "Train Epoch: 43\tTotal_its: 1.05M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.2292\tTime: 23.78\tLR: 1.0000\n",
      "Train Epoch: 43\tTotal_its: 1.05M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.3253\tTime: 23.89\tLR: 1.0000\n",
      "Validation Loss: 4.8336\tPerp: 125.6567\n",
      "Train Epoch: 44\tTotal_its: 1.06M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.6442\tTime: 24.01\tLR: 1.0000\n",
      "Train Epoch: 44\tTotal_its: 1.06M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.3586\tTime: 24.12\tLR: 1.0000\n",
      "Train Epoch: 44\tTotal_its: 1.07M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.2440\tTime: 24.22\tLR: 1.0000\n",
      "Train Epoch: 44\tTotal_its: 1.07M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.2430\tTime: 24.32\tLR: 1.0000\n",
      "Train Epoch: 44\tTotal_its: 1.08M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.2421\tTime: 24.43\tLR: 1.0000\n",
      "Validation Loss: 4.8413\tPerp: 126.6347\n",
      "The end! 24.55 minutes\n"
     ]
    }
   ],
   "source": [
    "qnet.modelvals['adas']=0 #no adaptive softmax\n",
    "qnet.model_fit()\n",
    "traditional_val_loss_list=qnet.modelvals['val_loss_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run QRNN-network with adaptive softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the same RNNnet w/ adaptive softmax\n",
    "class QRNNnet_adasoftmax(nn.Module):\n",
    "    def __init__(self, vs, emb_sz, nh, nl,bs):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vs, emb_sz)\n",
    "        self.rnn = QRNN(emb_sz, nh, num_layers=nl)\n",
    "        self.rnn.cuda()\n",
    "        self.hidden=self.one_hidden()\n",
    "        self.out=nn.AdaptiveLogSoftmaxWithLoss(nh, vs, cutoffs=[round(vs/15),3*round(vs/15)],div_value=4)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        #set target to [1:] of the sequence (i.e. all but the first value)\n",
    "        #set x to [:-1] of the sequence (i.e. all but the last value)\n",
    "        target=x[1:,:] #[bptt,bs]\n",
    "        target=target.view(target.size()[0]*target.size()[1])#[bptt*bs]\n",
    "        x=x[:-1,:] #[bptt,bs]\n",
    "        x = self.embed(x) # x -> [bptt,bs] -> [bptt,bs,emb_sz] \n",
    "        x,new_h = self.rnn(x, self.hidden) #[bptt,bs,emb_sz]  -> [bptt,bs,nh]\n",
    "        self.hidden = Variable(new_h.data)#repackage\n",
    "        x=x.view(-1,x.size()[2])#[bptt*bs,nh]\n",
    "        return self.out(x,target)\n",
    "         \n",
    "    def one_hidden(self,nl,bs,nh):\n",
    "        hidden1=Variable(torch.zeros(nl, bs, nh).cuda(), requires_grad=False)\n",
    "        return hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QRNNnet_adasoftmax(\n",
      "  (embed): Embedding(25520, 300)\n",
      "  (rnn): QRNN(\n",
      "    (layers): ModuleList(\n",
      "      (0): QRNNLayer(\n",
      "        (linear): Linear(in_features=300, out_features=900, bias=True)\n",
      "      )\n",
      "      (1): QRNNLayer(\n",
      "        (linear): Linear(in_features=300, out_features=900, bias=True)\n",
      "      )\n",
      "      (2): QRNNLayer(\n",
      "        (linear): Linear(in_features=300, out_features=900, bias=True)\n",
      "      )\n",
      "      (3): QRNNLayer(\n",
      "        (linear): Linear(in_features=300, out_features=900, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (out): AdaptiveLogSoftmaxWithLoss(\n",
      "    (head): Linear(in_features=300, out_features=1703, bias=False)\n",
      "    (tail): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=300, out_features=75, bias=False)\n",
      "        (1): Linear(in_features=75, out_features=3402, bias=False)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=300, out_features=18, bias=False)\n",
      "        (1): Linear(in_features=18, out_features=20417, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#create QRNN network w/ adaptive softmax\n",
    "qnet=modeler(data_lm.train_dl,data_lm.valid_dl,\n",
    "                           QRNNnet_adasoftmax(vs, emb_sz,nh,nl,bs),criterion,modelvals)\n",
    "print(qnet.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_w=np.load(DATAPATH/'emb_wgts300_proc.npy') #load embedding weights\n",
    "qnet.model.embed.weight.data=torch.FloatTensor(new_w).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1131: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:1143: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\tTotal_its: 0.00M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 9.4836\tTime: 0.00\tLR: 1.0000\n",
      "Train Epoch: 0\tTotal_its: 0.01M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 7.0893\tTime: 0.04\tLR: 1.0000\n",
      "Train Epoch: 0\tTotal_its: 0.01M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 6.9375\tTime: 0.07\tLR: 1.0000\n",
      "Train Epoch: 0\tTotal_its: 0.02M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 6.9278\tTime: 0.10\tLR: 1.0000\n",
      "Train Epoch: 0\tTotal_its: 0.02M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.9641\tTime: 0.13\tLR: 1.0000\n",
      "Validation Loss: 6.6811\tPerp: 797.2305\n",
      "Train Epoch: 1\tTotal_its: 0.02M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.9149\tTime: 0.17\tLR: 1.0000\n",
      "Train Epoch: 1\tTotal_its: 0.03M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 6.8345\tTime: 0.21\tLR: 1.0000\n",
      "Train Epoch: 1\tTotal_its: 0.03M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 6.8533\tTime: 0.24\tLR: 1.0000\n",
      "Train Epoch: 1\tTotal_its: 0.04M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 6.8523\tTime: 0.28\tLR: 1.0000\n",
      "Train Epoch: 1\tTotal_its: 0.04M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.9028\tTime: 0.31\tLR: 1.0000\n",
      "Validation Loss: 6.5670\tPerp: 711.2229\n",
      "Train Epoch: 2\tTotal_its: 0.05M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.8202\tTime: 0.35\tLR: 1.0000\n",
      "Train Epoch: 2\tTotal_its: 0.05M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 6.8339\tTime: 0.38\tLR: 1.0000\n",
      "Train Epoch: 2\tTotal_its: 0.06M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 6.9207\tTime: 0.42\tLR: 1.0000\n",
      "Train Epoch: 2\tTotal_its: 0.06M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 6.8496\tTime: 0.45\tLR: 1.0000\n",
      "Train Epoch: 2\tTotal_its: 0.07M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.6695\tTime: 0.48\tLR: 1.0000\n",
      "Validation Loss: 6.3969\tPerp: 599.9933\n",
      "Train Epoch: 3\tTotal_its: 0.07M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.7080\tTime: 0.52\tLR: 1.0000\n",
      "Train Epoch: 3\tTotal_its: 0.08M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 6.5392\tTime: 0.55\tLR: 1.0000\n",
      "Train Epoch: 3\tTotal_its: 0.08M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 6.6321\tTime: 0.59\tLR: 1.0000\n",
      "Train Epoch: 3\tTotal_its: 0.09M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 6.4845\tTime: 0.62\tLR: 1.0000\n",
      "Train Epoch: 3\tTotal_its: 0.09M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.5003\tTime: 0.65\tLR: 1.0000\n",
      "Validation Loss: 6.1422\tPerp: 465.0869\n",
      "Train Epoch: 4\tTotal_its: 0.10M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.5380\tTime: 0.69\tLR: 1.0000\n",
      "Train Epoch: 4\tTotal_its: 0.10M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 6.4168\tTime: 0.73\tLR: 1.0000\n",
      "Train Epoch: 4\tTotal_its: 0.11M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 6.2053\tTime: 0.76\tLR: 1.0000\n",
      "Train Epoch: 4\tTotal_its: 0.11M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 6.3184\tTime: 0.79\tLR: 1.0000\n",
      "Train Epoch: 4\tTotal_its: 0.12M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.2439\tTime: 0.83\tLR: 1.0000\n",
      "Validation Loss: 5.9518\tPerp: 384.4529\n",
      "Train Epoch: 5\tTotal_its: 0.12M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.1892\tTime: 0.87\tLR: 1.0000\n",
      "Train Epoch: 5\tTotal_its: 0.13M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 6.1262\tTime: 0.90\tLR: 1.0000\n",
      "Train Epoch: 5\tTotal_its: 0.13M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 6.0738\tTime: 0.93\tLR: 1.0000\n",
      "Train Epoch: 5\tTotal_its: 0.14M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 6.1049\tTime: 0.97\tLR: 1.0000\n",
      "Train Epoch: 5\tTotal_its: 0.14M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 6.0522\tTime: 1.00\tLR: 1.0000\n",
      "Validation Loss: 5.7834\tPerp: 324.8549\n",
      "Train Epoch: 6\tTotal_its: 0.14M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 6.0933\tTime: 1.04\tLR: 1.0000\n",
      "Train Epoch: 6\tTotal_its: 0.15M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 6.0072\tTime: 1.08\tLR: 1.0000\n",
      "Train Epoch: 6\tTotal_its: 0.15M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.9291\tTime: 1.11\tLR: 1.0000\n",
      "Train Epoch: 6\tTotal_its: 0.16M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.8905\tTime: 1.14\tLR: 1.0000\n",
      "Train Epoch: 6\tTotal_its: 0.16M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.6844\tTime: 1.18\tLR: 1.0000\n",
      "Validation Loss: 5.6513\tPerp: 284.6669\n",
      "Train Epoch: 7\tTotal_its: 0.17M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.8398\tTime: 1.22\tLR: 1.0000\n",
      "Train Epoch: 7\tTotal_its: 0.17M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.7666\tTime: 1.25\tLR: 1.0000\n",
      "Train Epoch: 7\tTotal_its: 0.18M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.7564\tTime: 1.28\tLR: 1.0000\n",
      "Train Epoch: 7\tTotal_its: 0.18M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.8226\tTime: 1.32\tLR: 1.0000\n",
      "Train Epoch: 7\tTotal_its: 0.19M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.8471\tTime: 1.35\tLR: 1.0000\n",
      "Validation Loss: 5.5510\tPerp: 257.5049\n",
      "Train Epoch: 8\tTotal_its: 0.19M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.7369\tTime: 1.39\tLR: 1.0000\n",
      "Train Epoch: 8\tTotal_its: 0.20M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.6638\tTime: 1.42\tLR: 1.0000\n",
      "Train Epoch: 8\tTotal_its: 0.20M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.6119\tTime: 1.45\tLR: 1.0000\n",
      "Train Epoch: 8\tTotal_its: 0.21M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.7345\tTime: 1.49\tLR: 1.0000\n",
      "Train Epoch: 8\tTotal_its: 0.21M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.7160\tTime: 1.52\tLR: 1.0000\n",
      "Validation Loss: 5.4594\tPerp: 234.9541\n",
      "Train Epoch: 9\tTotal_its: 0.22M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.6745\tTime: 1.56\tLR: 1.0000\n",
      "Train Epoch: 9\tTotal_its: 0.22M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.5350\tTime: 1.59\tLR: 1.0000\n",
      "Train Epoch: 9\tTotal_its: 0.23M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.5221\tTime: 1.63\tLR: 1.0000\n",
      "Train Epoch: 9\tTotal_its: 0.23M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.5016\tTime: 1.66\tLR: 1.0000\n",
      "Train Epoch: 9\tTotal_its: 0.24M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.4767\tTime: 1.69\tLR: 1.0000\n",
      "Validation Loss: 5.4035\tPerp: 222.1751\n",
      "Train Epoch: 10\tTotal_its: 0.24M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.6342\tTime: 1.73\tLR: 1.0000\n",
      "Train Epoch: 10\tTotal_its: 0.25M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.4684\tTime: 1.77\tLR: 1.0000\n",
      "Train Epoch: 10\tTotal_its: 0.25M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.4331\tTime: 1.80\tLR: 1.0000\n",
      "Train Epoch: 10\tTotal_its: 0.26M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.5504\tTime: 1.83\tLR: 1.0000\n",
      "Train Epoch: 10\tTotal_its: 0.26M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.4860\tTime: 1.87\tLR: 1.0000\n",
      "Validation Loss: 5.3642\tPerp: 213.6271\n",
      "Train Epoch: 11\tTotal_its: 0.26M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.6867\tTime: 1.91\tLR: 1.0000\n",
      "Train Epoch: 11\tTotal_its: 0.27M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.4799\tTime: 1.94\tLR: 1.0000\n",
      "Train Epoch: 11\tTotal_its: 0.27M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.3350\tTime: 1.97\tLR: 1.0000\n",
      "Train Epoch: 11\tTotal_its: 0.28M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.3593\tTime: 2.01\tLR: 1.0000\n",
      "Train Epoch: 11\tTotal_its: 0.28M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.3860\tTime: 2.04\tLR: 1.0000\n",
      "Validation Loss: 5.3063\tPerp: 201.6080\n",
      "Train Epoch: 12\tTotal_its: 0.29M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.6237\tTime: 2.08\tLR: 1.0000\n",
      "Train Epoch: 12\tTotal_its: 0.29M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.2917\tTime: 2.11\tLR: 1.0000\n",
      "Train Epoch: 12\tTotal_its: 0.30M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.3793\tTime: 2.15\tLR: 1.0000\n",
      "Train Epoch: 12\tTotal_its: 0.30M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.4420\tTime: 2.18\tLR: 1.0000\n",
      "Train Epoch: 12\tTotal_its: 0.31M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.3115\tTime: 2.21\tLR: 1.0000\n",
      "Validation Loss: 5.2715\tPerp: 194.7025\n",
      "Train Epoch: 13\tTotal_its: 0.31M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.5052\tTime: 2.25\tLR: 1.0000\n",
      "Train Epoch: 13\tTotal_its: 0.32M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.2476\tTime: 2.29\tLR: 1.0000\n",
      "Train Epoch: 13\tTotal_its: 0.32M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.2779\tTime: 2.32\tLR: 1.0000\n",
      "Train Epoch: 13\tTotal_its: 0.33M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.2165\tTime: 2.35\tLR: 1.0000\n",
      "Train Epoch: 13\tTotal_its: 0.33M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.2860\tTime: 2.39\tLR: 1.0000\n",
      "Validation Loss: 5.2148\tPerp: 183.9795\n",
      "Train Epoch: 14\tTotal_its: 0.34M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.5084\tTime: 2.43\tLR: 1.0000\n",
      "Train Epoch: 14\tTotal_its: 0.34M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.1651\tTime: 2.46\tLR: 1.0000\n",
      "Train Epoch: 14\tTotal_its: 0.35M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.2770\tTime: 2.49\tLR: 1.0000\n",
      "Train Epoch: 14\tTotal_its: 0.35M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.1302\tTime: 2.53\tLR: 1.0000\n",
      "Train Epoch: 14\tTotal_its: 0.36M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.1811\tTime: 2.56\tLR: 1.0000\n",
      "Validation Loss: 5.1897\tPerp: 179.4186\n",
      "Train Epoch: 15\tTotal_its: 0.36M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.4681\tTime: 2.60\tLR: 1.0000\n",
      "Train Epoch: 15\tTotal_its: 0.37M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.1921\tTime: 2.63\tLR: 1.0000\n",
      "Train Epoch: 15\tTotal_its: 0.37M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.2285\tTime: 2.67\tLR: 1.0000\n",
      "Train Epoch: 15\tTotal_its: 0.38M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.1978\tTime: 2.70\tLR: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15\tTotal_its: 0.38M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.2070\tTime: 2.73\tLR: 1.0000\n",
      "Validation Loss: 5.1593\tPerp: 174.0504\n",
      "Train Epoch: 16\tTotal_its: 0.38M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.4168\tTime: 2.77\tLR: 1.0000\n",
      "Train Epoch: 16\tTotal_its: 0.39M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.2277\tTime: 2.81\tLR: 1.0000\n",
      "Train Epoch: 16\tTotal_its: 0.39M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.1174\tTime: 2.84\tLR: 1.0000\n",
      "Train Epoch: 16\tTotal_its: 0.40M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.0854\tTime: 2.87\tLR: 1.0000\n",
      "Train Epoch: 16\tTotal_its: 0.40M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.2011\tTime: 2.91\tLR: 1.0000\n",
      "Validation Loss: 5.1328\tPerp: 169.4911\n",
      "Train Epoch: 17\tTotal_its: 0.41M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.2487\tTime: 2.95\tLR: 1.0000\n",
      "Train Epoch: 17\tTotal_its: 0.41M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.1759\tTime: 2.98\tLR: 1.0000\n",
      "Train Epoch: 17\tTotal_its: 0.42M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.1313\tTime: 3.01\tLR: 1.0000\n",
      "Train Epoch: 17\tTotal_its: 0.42M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 5.2314\tTime: 3.05\tLR: 1.0000\n",
      "Train Epoch: 17\tTotal_its: 0.43M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.1509\tTime: 3.08\tLR: 1.0000\n",
      "Validation Loss: 5.1021\tPerp: 164.3606\n",
      "Train Epoch: 18\tTotal_its: 0.43M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.2847\tTime: 3.12\tLR: 1.0000\n",
      "Train Epoch: 18\tTotal_its: 0.44M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 5.0327\tTime: 3.15\tLR: 1.0000\n",
      "Train Epoch: 18\tTotal_its: 0.44M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 5.1214\tTime: 3.19\tLR: 1.0000\n",
      "Train Epoch: 18\tTotal_its: 0.45M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.9525\tTime: 3.22\tLR: 1.0000\n",
      "Train Epoch: 18\tTotal_its: 0.45M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.8431\tTime: 3.25\tLR: 1.0000\n",
      "Validation Loss: 5.0840\tPerp: 161.4229\n",
      "Train Epoch: 19\tTotal_its: 0.46M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.1214\tTime: 3.29\tLR: 1.0000\n",
      "Train Epoch: 19\tTotal_its: 0.46M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.9543\tTime: 3.33\tLR: 1.0000\n",
      "Train Epoch: 19\tTotal_its: 0.47M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.9441\tTime: 3.36\tLR: 1.0000\n",
      "Train Epoch: 19\tTotal_its: 0.47M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.9171\tTime: 3.39\tLR: 1.0000\n",
      "Train Epoch: 19\tTotal_its: 0.48M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.0187\tTime: 3.43\tLR: 1.0000\n",
      "Validation Loss: 5.0611\tPerp: 157.7653\n",
      "Train Epoch: 20\tTotal_its: 0.48M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.1648\tTime: 3.47\tLR: 1.0000\n",
      "Train Epoch: 20\tTotal_its: 0.49M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.8849\tTime: 3.50\tLR: 1.0000\n",
      "Train Epoch: 20\tTotal_its: 0.49M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.9399\tTime: 3.54\tLR: 1.0000\n",
      "Train Epoch: 20\tTotal_its: 0.50M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.9559\tTime: 3.57\tLR: 1.0000\n",
      "Train Epoch: 20\tTotal_its: 0.50M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.0051\tTime: 3.60\tLR: 1.0000\n",
      "Validation Loss: 5.0453\tPerp: 155.2849\n",
      "Train Epoch: 21\tTotal_its: 0.50M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.2670\tTime: 3.64\tLR: 1.0000\n",
      "Train Epoch: 21\tTotal_its: 0.51M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.7845\tTime: 3.68\tLR: 1.0000\n",
      "Train Epoch: 21\tTotal_its: 0.51M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.8411\tTime: 3.71\tLR: 1.0000\n",
      "Train Epoch: 21\tTotal_its: 0.52M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.8012\tTime: 3.74\tLR: 1.0000\n",
      "Train Epoch: 21\tTotal_its: 0.52M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.8373\tTime: 3.78\tLR: 1.0000\n",
      "Validation Loss: 5.0300\tPerp: 152.9357\n",
      "Train Epoch: 22\tTotal_its: 0.53M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.1498\tTime: 3.82\tLR: 1.0000\n",
      "Train Epoch: 22\tTotal_its: 0.53M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.9521\tTime: 3.85\tLR: 1.0000\n",
      "Train Epoch: 22\tTotal_its: 0.54M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.9803\tTime: 3.88\tLR: 1.0000\n",
      "Train Epoch: 22\tTotal_its: 0.54M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.8162\tTime: 3.92\tLR: 1.0000\n",
      "Train Epoch: 22\tTotal_its: 0.55M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 5.0002\tTime: 3.95\tLR: 1.0000\n",
      "Validation Loss: 4.9936\tPerp: 147.4735\n",
      "Train Epoch: 23\tTotal_its: 0.55M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.1504\tTime: 3.99\tLR: 1.0000\n",
      "Train Epoch: 23\tTotal_its: 0.56M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.7297\tTime: 4.02\tLR: 1.0000\n",
      "Train Epoch: 23\tTotal_its: 0.56M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.6721\tTime: 4.06\tLR: 1.0000\n",
      "Train Epoch: 23\tTotal_its: 0.57M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.8472\tTime: 4.09\tLR: 1.0000\n",
      "Train Epoch: 23\tTotal_its: 0.57M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.8007\tTime: 4.12\tLR: 1.0000\n",
      "Validation Loss: 4.9810\tPerp: 145.6137\n",
      "Train Epoch: 24\tTotal_its: 0.58M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.0798\tTime: 4.16\tLR: 1.0000\n",
      "Train Epoch: 24\tTotal_its: 0.58M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.8566\tTime: 4.20\tLR: 1.0000\n",
      "Train Epoch: 24\tTotal_its: 0.59M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.7199\tTime: 4.23\tLR: 1.0000\n",
      "Train Epoch: 24\tTotal_its: 0.59M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.7028\tTime: 4.26\tLR: 1.0000\n",
      "Train Epoch: 24\tTotal_its: 0.60M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.7599\tTime: 4.30\tLR: 1.0000\n",
      "Validation Loss: 4.9830\tPerp: 145.9057\n",
      "Train Epoch: 25\tTotal_its: 0.60M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.9725\tTime: 4.34\tLR: 1.0000\n",
      "Train Epoch: 25\tTotal_its: 0.61M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.8071\tTime: 4.37\tLR: 1.0000\n",
      "Train Epoch: 25\tTotal_its: 0.61M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.6920\tTime: 4.40\tLR: 1.0000\n",
      "Train Epoch: 25\tTotal_its: 0.62M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.7293\tTime: 4.44\tLR: 1.0000\n",
      "Train Epoch: 25\tTotal_its: 0.62M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.7122\tTime: 4.47\tLR: 1.0000\n",
      "Validation Loss: 4.9527\tPerp: 141.5607\n",
      "Train Epoch: 26\tTotal_its: 0.62M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.0700\tTime: 4.51\tLR: 1.0000\n",
      "Train Epoch: 26\tTotal_its: 0.63M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.6755\tTime: 4.54\tLR: 1.0000\n",
      "Train Epoch: 26\tTotal_its: 0.63M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.7897\tTime: 4.58\tLR: 1.0000\n",
      "Train Epoch: 26\tTotal_its: 0.64M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.7391\tTime: 4.61\tLR: 1.0000\n",
      "Train Epoch: 26\tTotal_its: 0.64M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.7287\tTime: 4.64\tLR: 1.0000\n",
      "Validation Loss: 4.9337\tPerp: 138.8965\n",
      "Train Epoch: 27\tTotal_its: 0.65M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.0224\tTime: 4.68\tLR: 1.0000\n",
      "Train Epoch: 27\tTotal_its: 0.65M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.6067\tTime: 4.72\tLR: 1.0000\n",
      "Train Epoch: 27\tTotal_its: 0.66M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.5517\tTime: 4.75\tLR: 1.0000\n",
      "Train Epoch: 27\tTotal_its: 0.66M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.5323\tTime: 4.78\tLR: 1.0000\n",
      "Train Epoch: 27\tTotal_its: 0.67M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.7070\tTime: 4.82\tLR: 1.0000\n",
      "Validation Loss: 4.9227\tPerp: 137.3724\n",
      "Train Epoch: 28\tTotal_its: 0.67M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.0009\tTime: 4.86\tLR: 1.0000\n",
      "Train Epoch: 28\tTotal_its: 0.68M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.4519\tTime: 4.89\tLR: 1.0000\n",
      "Train Epoch: 28\tTotal_its: 0.68M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.7284\tTime: 4.92\tLR: 1.0000\n",
      "Train Epoch: 28\tTotal_its: 0.69M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.6900\tTime: 4.96\tLR: 1.0000\n",
      "Train Epoch: 28\tTotal_its: 0.69M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5733\tTime: 4.99\tLR: 1.0000\n",
      "Validation Loss: 4.9132\tPerp: 136.0676\n",
      "Train Epoch: 29\tTotal_its: 0.70M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.8886\tTime: 5.03\tLR: 1.0000\n",
      "Train Epoch: 29\tTotal_its: 0.70M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.6122\tTime: 5.06\tLR: 1.0000\n",
      "Train Epoch: 29\tTotal_its: 0.71M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.5640\tTime: 5.10\tLR: 1.0000\n",
      "Train Epoch: 29\tTotal_its: 0.71M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.5642\tTime: 5.13\tLR: 1.0000\n",
      "Train Epoch: 29\tTotal_its: 0.72M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5852\tTime: 5.16\tLR: 1.0000\n",
      "Validation Loss: 4.9084\tPerp: 135.4224\n",
      "Train Epoch: 30\tTotal_its: 0.72M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.0907\tTime: 5.20\tLR: 1.0000\n",
      "Train Epoch: 30\tTotal_its: 0.73M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.6958\tTime: 5.24\tLR: 1.0000\n",
      "Train Epoch: 30\tTotal_its: 0.73M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.4282\tTime: 5.27\tLR: 1.0000\n",
      "Train Epoch: 30\tTotal_its: 0.74M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.5269\tTime: 5.30\tLR: 1.0000\n",
      "Train Epoch: 30\tTotal_its: 0.74M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5500\tTime: 5.34\tLR: 1.0000\n",
      "Validation Loss: 4.9050\tPerp: 134.9616\n",
      "Train Epoch: 31\tTotal_its: 0.74M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 5.1562\tTime: 5.38\tLR: 1.0000\n",
      "Train Epoch: 31\tTotal_its: 0.75M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.5614\tTime: 5.41\tLR: 1.0000\n",
      "Train Epoch: 31\tTotal_its: 0.75M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.4687\tTime: 5.44\tLR: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 31\tTotal_its: 0.76M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4227\tTime: 5.48\tLR: 1.0000\n",
      "Train Epoch: 31\tTotal_its: 0.76M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5296\tTime: 5.51\tLR: 1.0000\n",
      "Validation Loss: 4.8899\tPerp: 132.9404\n",
      "Train Epoch: 32\tTotal_its: 0.77M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.7265\tTime: 5.55\tLR: 1.0000\n",
      "Train Epoch: 32\tTotal_its: 0.77M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.5414\tTime: 5.58\tLR: 1.0000\n",
      "Train Epoch: 32\tTotal_its: 0.78M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.6362\tTime: 5.62\tLR: 1.0000\n",
      "Train Epoch: 32\tTotal_its: 0.78M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4471\tTime: 5.65\tLR: 1.0000\n",
      "Train Epoch: 32\tTotal_its: 0.79M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.4548\tTime: 5.68\tLR: 1.0000\n",
      "Validation Loss: 4.8986\tPerp: 134.1023\n",
      "Train Epoch: 33\tTotal_its: 0.79M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.7994\tTime: 5.72\tLR: 1.0000\n",
      "Train Epoch: 33\tTotal_its: 0.80M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.4913\tTime: 5.76\tLR: 1.0000\n",
      "Train Epoch: 33\tTotal_its: 0.80M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.4063\tTime: 5.79\tLR: 1.0000\n",
      "Train Epoch: 33\tTotal_its: 0.81M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4472\tTime: 5.83\tLR: 1.0000\n",
      "Train Epoch: 33\tTotal_its: 0.81M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5028\tTime: 5.86\tLR: 1.0000\n",
      "Validation Loss: 4.8866\tPerp: 132.5004\n",
      "Train Epoch: 34\tTotal_its: 0.82M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.8550\tTime: 5.90\tLR: 1.0000\n",
      "Train Epoch: 34\tTotal_its: 0.82M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.3415\tTime: 5.93\tLR: 1.0000\n",
      "Train Epoch: 34\tTotal_its: 0.83M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.4604\tTime: 5.97\tLR: 1.0000\n",
      "Train Epoch: 34\tTotal_its: 0.83M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4435\tTime: 6.00\tLR: 1.0000\n",
      "Train Epoch: 34\tTotal_its: 0.84M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.4266\tTime: 6.03\tLR: 1.0000\n",
      "Validation Loss: 4.8675\tPerp: 129.9984\n",
      "Train Epoch: 35\tTotal_its: 0.84M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.8280\tTime: 6.07\tLR: 1.0000\n",
      "Train Epoch: 35\tTotal_its: 0.85M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.4104\tTime: 6.11\tLR: 1.0000\n",
      "Train Epoch: 35\tTotal_its: 0.85M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.6346\tTime: 6.14\tLR: 1.0000\n",
      "Train Epoch: 35\tTotal_its: 0.86M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4581\tTime: 6.17\tLR: 1.0000\n",
      "Train Epoch: 35\tTotal_its: 0.86M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.5588\tTime: 6.21\tLR: 1.0000\n",
      "Validation Loss: 4.8685\tPerp: 130.1207\n",
      "Train Epoch: 36\tTotal_its: 0.86M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.9554\tTime: 6.25\tLR: 1.0000\n",
      "Train Epoch: 36\tTotal_its: 0.87M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.3606\tTime: 6.28\tLR: 1.0000\n",
      "Train Epoch: 36\tTotal_its: 0.87M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.4430\tTime: 6.31\tLR: 1.0000\n",
      "Train Epoch: 36\tTotal_its: 0.88M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.3563\tTime: 6.35\tLR: 1.0000\n",
      "Train Epoch: 36\tTotal_its: 0.88M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.4545\tTime: 6.38\tLR: 1.0000\n",
      "Validation Loss: 4.8720\tPerp: 130.5851\n",
      "Train Epoch: 37\tTotal_its: 0.89M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.8167\tTime: 6.42\tLR: 1.0000\n",
      "Train Epoch: 37\tTotal_its: 0.89M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.4053\tTime: 6.45\tLR: 1.0000\n",
      "Train Epoch: 37\tTotal_its: 0.90M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.2884\tTime: 6.49\tLR: 1.0000\n",
      "Train Epoch: 37\tTotal_its: 0.90M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.3258\tTime: 6.52\tLR: 1.0000\n",
      "Train Epoch: 37\tTotal_its: 0.91M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.4694\tTime: 6.55\tLR: 1.0000\n",
      "Validation Loss: 4.8595\tPerp: 128.9604\n",
      "Train Epoch: 38\tTotal_its: 0.91M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.8554\tTime: 6.59\tLR: 1.0000\n",
      "Train Epoch: 38\tTotal_its: 0.92M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.3464\tTime: 6.63\tLR: 1.0000\n",
      "Train Epoch: 38\tTotal_its: 0.92M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.5049\tTime: 6.66\tLR: 1.0000\n",
      "Train Epoch: 38\tTotal_its: 0.93M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.3955\tTime: 6.69\tLR: 1.0000\n",
      "Train Epoch: 38\tTotal_its: 0.93M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.3812\tTime: 6.73\tLR: 1.0000\n",
      "Validation Loss: 4.8697\tPerp: 130.2817\n",
      "Train Epoch: 39\tTotal_its: 0.94M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.7592\tTime: 6.77\tLR: 1.0000\n",
      "Train Epoch: 39\tTotal_its: 0.94M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.2822\tTime: 6.80\tLR: 1.0000\n",
      "Train Epoch: 39\tTotal_its: 0.95M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.2663\tTime: 6.83\tLR: 1.0000\n",
      "Train Epoch: 39\tTotal_its: 0.95M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.5292\tTime: 6.87\tLR: 1.0000\n",
      "Train Epoch: 39\tTotal_its: 0.96M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.2719\tTime: 6.90\tLR: 1.0000\n",
      "Validation Loss: 4.8666\tPerp: 129.8721\n",
      "Train Epoch: 40\tTotal_its: 0.96M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.8798\tTime: 6.94\tLR: 1.0000\n",
      "Train Epoch: 40\tTotal_its: 0.97M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.2638\tTime: 6.97\tLR: 1.0000\n",
      "Train Epoch: 40\tTotal_its: 0.97M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.3123\tTime: 7.01\tLR: 1.0000\n",
      "Train Epoch: 40\tTotal_its: 0.98M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.3101\tTime: 7.04\tLR: 1.0000\n",
      "Train Epoch: 40\tTotal_its: 0.98M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.2431\tTime: 7.07\tLR: 1.0000\n",
      "Validation Loss: 4.8728\tPerp: 130.6880\n",
      "Train Epoch: 41\tTotal_its: 0.98M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.8901\tTime: 7.11\tLR: 1.0000\n",
      "Train Epoch: 41\tTotal_its: 0.99M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.2878\tTime: 7.15\tLR: 1.0000\n",
      "Train Epoch: 41\tTotal_its: 0.99M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.3230\tTime: 7.18\tLR: 1.0000\n",
      "Train Epoch: 41\tTotal_its: 1.00M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.3960\tTime: 7.21\tLR: 1.0000\n",
      "Train Epoch: 41\tTotal_its: 1.00M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.1023\tTime: 7.25\tLR: 1.0000\n",
      "Validation Loss: 4.8657\tPerp: 129.7574\n",
      "Train Epoch: 42\tTotal_its: 1.01M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.6810\tTime: 7.29\tLR: 1.0000\n",
      "Train Epoch: 42\tTotal_its: 1.01M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.1255\tTime: 7.32\tLR: 1.0000\n",
      "Train Epoch: 42\tTotal_its: 1.02M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.1856\tTime: 7.35\tLR: 1.0000\n",
      "Train Epoch: 42\tTotal_its: 1.02M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.4612\tTime: 7.39\tLR: 1.0000\n",
      "Train Epoch: 42\tTotal_its: 1.03M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.3202\tTime: 7.42\tLR: 1.0000\n",
      "Validation Loss: 4.8721\tPerp: 130.5953\n",
      "Train Epoch: 43\tTotal_its: 1.03M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.8102\tTime: 7.46\tLR: 1.0000\n",
      "Train Epoch: 43\tTotal_its: 1.04M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.2120\tTime: 7.49\tLR: 1.0000\n",
      "Train Epoch: 43\tTotal_its: 1.04M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.4037\tTime: 7.53\tLR: 1.0000\n",
      "Train Epoch: 43\tTotal_its: 1.05M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.1934\tTime: 7.56\tLR: 1.0000\n",
      "Train Epoch: 43\tTotal_its: 1.05M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.2655\tTime: 7.59\tLR: 1.0000\n",
      "Validation Loss: 4.8628\tPerp: 129.3860\n",
      "Train Epoch: 44\tTotal_its: 1.06M [0.00M/0.02M]\tPercdone: 0.00\tLoss: 4.8045\tTime: 7.63\tLR: 1.0000\n",
      "Train Epoch: 44\tTotal_its: 1.06M [0.01M/0.02M]\tPercdone: 0.21\tLoss: 4.2313\tTime: 7.67\tLR: 1.0000\n",
      "Train Epoch: 44\tTotal_its: 1.07M [0.01M/0.02M]\tPercdone: 0.42\tLoss: 4.3611\tTime: 7.70\tLR: 1.0000\n",
      "Train Epoch: 44\tTotal_its: 1.07M [0.02M/0.02M]\tPercdone: 0.63\tLoss: 4.3245\tTime: 7.73\tLR: 1.0000\n",
      "Train Epoch: 44\tTotal_its: 1.08M [0.02M/0.02M]\tPercdone: 0.84\tLoss: 4.3431\tTime: 7.77\tLR: 1.0000\n",
      "Validation Loss: 4.8800\tPerp: 131.6305\n",
      "The end! 7.80 minutes\n"
     ]
    }
   ],
   "source": [
    "qnet.modelvals['adas']=1 #turn on adaptive softmax\n",
    "qnet.model_fit()\n",
    "adaptive_val_loss_list=qnet.modelvals['val_loss_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_list,trad_loss_list,trad_perp_list,trad_time_list=zip(*traditional_val_loss_list)\n",
    "epoch_list,adas_loss_list,adas_perp_list,adas_time_list=zip(*adaptive_val_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set() #set to seaborn styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_list,trad_loss_list,'b',label='traditional')\n",
    "plt.plot(epoch_list,adas_loss_list,'r',label='adaptive')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trad_time_list,trad_loss_list,'b',label='traditional')\n",
    "plt.plot(adas_time_list,adas_loss_list,'r',label='adaptive')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Minutes')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
